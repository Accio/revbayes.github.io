<!doctype html>
<html lang="en">
<link rel="icon" type="image/png" href="/revbayes-site/assets/img/favicon.png" >
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="search-domain" value="https://revbayes.github.io/revbayes-site/">
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet">
    <link rel="stylesheet" href="/revbayes-site/assets/css/syntax.css">
    <link rel="stylesheet" type="text/css" href="/revbayes-site/assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes-site/assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes-site/assets/css/main.css" />
    <title>RevBayes: Model selection</title>
  </head>
  <body>
    <div class="container">
      <nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <a href="/revbayes-site/" class="pull-left">
        
        <img class="navbar-logo" src="/revbayes-site/assets/img/aquabayes-desaturated.png" alt="RevBayes Home" />
        
      </a>
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar" align="right"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">
        <li><a href="/revbayes-site/software">Software</a></li>
        <li><a href="/revbayes-site/tutorials/">Tutorials</a></li>
        <li><a href="/revbayes-site/workshops/">Workshops</a></li>
        <li><a href="/revbayes-site/developer/">Developer</a></li>
      </ul>
      <!-- <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form> -->
    </div>
  </div>
</nav>

      <div class="titlebar">
	<h1 class="maintitle">Model selection</h1>
	<h3 class="subtitle">Comparing relative model fit with Bayes factors</h3>
	<h4 class="authors">Sebastian Höhna, Michael J Landis, Tracy A Heath and Brian R Moore</h4>
</div>


<div class="sidebar no-print">
<blockquote class="overview" id="overview">
  <h2>Overview</h2>
  
  <div class="row">
    <div class="col-md-9">
        <strong>Prerequisites</strong>
        
          <ul id="prerequisites">
          
            <li><a href="/revbayes-site/tutorials/intro/">Rev Language Syntax</a></li>
          
            <li><a href="/revbayes-site/tutorials/intro_rev/">Basic introduction to Rev & MCMC</a></li>
          
            <li><a href="/revbayes-site/tutorials/mcmc_archery/">Introduction to MCMC using RevBayes</a></li>
          
            <li><a href="/revbayes-site/tutorials/mcmc_binomial/">Introduction to MCMC using RevBayes</a></li>
          
          </ul>
        
    </div>
  </div>
  
</blockquote>





<blockquote class="tutorial_files" id="tutorial_files">
    <h2>Data files and scripts</h2>
    
        
        <strong>Data Files</strong>
        <ul id="data_files">
        
        
          <li><a href="/revbayes-site/tutorials/model_selection_bayes_factors/data/primates_and_galeopterus_cytb.nex">primates_and_galeopterus_cytb.nex</a></li>
        
        </ul>
    
        
        <strong>Scripts</strong>
        <ul id="scripts">
        
        
          <li><a href="/revbayes-site/tutorials/model_selection_bayes_factors/scripts/marginal_likelihood_GTR_Gamma_inv.Rev">marginal_likelihood_GTR_Gamma_inv.Rev</a></li>
        
          <li><a href="/revbayes-site/tutorials/model_selection_bayes_factors/scripts/marginal_likelihood_JC.Rev">marginal_likelihood_JC.Rev</a></li>
        
        </ul>
    
</blockquote>


</div>


<h2>Required Software</h2>

For our tutorials we recommend that you download and install the latest release
of ‘RevBayes‘ <a href="#Hoehna2016b">(Höhna et al. 2016)</a>, which is available for Mac OS X, Windows,
and Linux operating systems. Directions for downloading and installing
the software are available on the program webpage:
<a href="http://revbayes.com">http://revbayes.com</a>. The exercises provided often also
require additional programs for editing text files and visualizing
output. The following are very useful tools for working with ‘RevBayes‘:

<ul>
<li>Some of the output can be automatically visualized using pre-made 
    <a href="https://www.r-project.org">R</a> functions provided by 
    our R-package <a href="https://github.com/revbayes/RevGadgets">RevGadgets</a>. 
    You should make sure that you have a recent release of 
    <a href="https://www.r-project.org">R</a> and 
    <a href="https://github.com/revbayes/RevGadgets">RevGadgets</a> installed.
</li>
<li>A good text editor – if you do not already have one that you like,
    we recommend one that has features for syntax coloring, easy
    navigation between different files, line numbers, etc. Good options
    include <a href="http://www.sublimetext.com/">Sublime Text</a>, 
    <a href="https://atom.io/">Atom</a> or <a href="https://notepad-plus-plus.org">NotePad++</a>, which are available for Mac OSX, Windows,
    and Linux.
</li>
<li><a href="http://tree.bio.ed.ac.uk/software/tracer/">Tracer</a> – for
    visualizing and assessing numerical parameter samples from
    ‘RevBayes‘
</li>
<li><a href="http://tgvaughan.github.io/icytree/">IcyTree</a> – a web-hosted
    phylogenetic tree visualization tool that is supported for
    <a href="https://www.mozilla.org/en-US/firefox/products/">Firefox</a> or
    <a href="https://www.google.com/chrome/">Google Chrome</a> browsers

<li><a href="http://tree.bio.ed.ac.uk/software/figtree/">FigTree</a> – a tree
    visualization program
</li>
</ul>



<h1 class="section" id="overview">Overview</h1>

<p>This tutorial provides the third protocol from our recent publication
<a href="#Hoehna2017a">(Höhna et al. 2017)</a>. The first protocol is described in the 
<a href="/tutorials/ctmc/">Substitution model tutorial</a>
and the second protocol is described in the 
<a href="https://github.com/revbayes/revbayes_tutorial/raw/master/tutorial_TeX/RB_Partition_Tutorial/RB_Partition_Tutorial.pdf">Partitioned data analysis tutorial</a>.</p>

<p>This tutorial demonstrates some general principles of Bayesian model
comparison, which is based on estimating the marginal likelihood of
competing models and then comparing their relative fit to the data using
Bayes factors. We consider the specific case of calculating Bayes
factors to select among different substitution models.</p>

<h1 class="section" id="introduction">Introduction</h1>

<p>For most sequence alignments, several (possibly many) substitution
models of varying complexity are plausible <em>a priori</em>. We therefore need
a way to objectively identify the model that balances estimation bias
and inflated error variance associated with under- and
over-parameterized models, respectively. Increasingly, model selection
is based on <em>Bayes factors</em> [<em>e.g.</em>, 
<a href="#Suchard2001">(Suchard et al. 2001; Lartillot 2006; Xie et al. 2011; Baele et al. 2012; Baele et al. 2013)</a>], which
involves first calculating the marginal likelihood of each candidate
model and then comparing the ratio of the marginal likelihoods for the
set of candidate models.</p>

<p>Given two models, $M_0$ and $M_1$, the Bayes-factor comparison assessing
the relative fit of each model to the data, $BF(M_0,M_1)$, is:</p>

<script type="math/tex; mode=display">BF(M_0,M_1) = \frac{\mbox{posterior odds}}{\mbox{prior odds}}.</script>

<p>The posterior odds is the posterior probability of $M_0$ given the data,
$\mathbf X$, divided by the posterior odds of $M_1$ given the data:</p>

<script type="math/tex; mode=display">\mbox{posterior odds} = \frac{\mathbb{P}(M_0 \mid \mathbf X)}{\mathbb{P}(M_1 \mid \mathbf X)},</script>

<p>and the prior odds is the prior probability of $M_0$ divided by the
prior probability of $M_1$:</p>

<script type="math/tex; mode=display">\mbox{prior odds} = \frac{\mathbb{P}(M_0)}{\mathbb{P}(M_1)}.</script>

<p>Thus, the Bayes factor measures the degree to which the data alter our belief
regarding the support for $M_0$ relative to $M_1$ <a href="#Lavine1999">(Lavine and Schervish 1999)</a>:</p>

<script type="math/tex; mode=display">\begin{equation}
BF(M_0,M_1) = \frac{\mathbb{P}(M_0 \mid \mathbf X, \theta_0)}{\mathbb{P}(M_1 \mid \mathbf X, \theta_1)} \div \frac{\mathbb{P}(M_0)}{\mathbb{P}(M_1)}. 
\tag{Bayes Factor}\label{eq:BF}
\end{equation}</script>

<p>Note that interpreting Bayes factors involves some subjectivity. That
is, it is up to <em>you</em> to decide the degree of your belief in $M_0$
relative to $M_1$. Despite the absence of an absolutely objective
model-selection threshold, we can refer to the scale [outlined by
<a href="#Jeffreys1961">(Jeffreys 1961)</a>] that provides a “rule-of-thumb” for interpreting these
measures (<a href="#tab_bf"></a>).</p>

<figure id="tab_bf"><table>
  <thead>
    <tr>
      <th style="text-align: right"><strong>Strength of evidence</strong></th>
      <th style="text-align: center">BF($M_0$,$M_1$)**</th>
      <th style="text-align: center"><strong>log(BF($M_0$,$M_1$))</strong></th>
      <th style="text-align: center"><strong>$log_{10}(BF(M_0$,$M_1))$</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">Negative (supports $M_1$)</td>
      <td style="text-align: center">$&lt;1$</td>
      <td style="text-align: center">$&lt;0$</td>
      <td style="text-align: center">$&lt;0$</td>
    </tr>
    <tr>
      <td style="text-align: right">Barely worth mentioning</td>
      <td style="text-align: center">$1$ to $3.2$</td>
      <td style="text-align: center">$0$ to $1.16$</td>
      <td style="text-align: center">$0$ to $0.5$</td>
    </tr>
    <tr>
      <td style="text-align: right">Substantial</td>
      <td style="text-align: center">$3.2$ to $10$</td>
      <td style="text-align: center">$1.16$ to $2.3$</td>
      <td style="text-align: center">$0.5$ to $1$</td>
    </tr>
    <tr>
      <td style="text-align: right">Strong</td>
      <td style="text-align: center">$10$ to $100$</td>
      <td style="text-align: center">$2.3$  to $4.6$</td>
      <td style="text-align: center">$1$ to $2$</td>
    </tr>
    <tr>
      <td style="text-align: right">Decisive</td>
      <td style="text-align: center">$&gt;100$</td>
      <td style="text-align: center">$&gt;4.6$</td>
      <td style="text-align: center">$&gt;2$</td>
    </tr>
  </tbody>
</table>

<figcaption>The scale for interpreting Bayes factors by Harold <a href="#Jeffreys1961">(Jeffreys 1961)</a>.</figcaption>
</figure>

<p>Unfortunately, it is generally not possible to directly calculate the
posterior odds to prior odds ratios. However, we can further define the
posterior odds ratio as:</p>

<script type="math/tex; mode=display">\begin{aligned}
\frac{\mathbb{P}(M_0 \mid \mathbf X)}{\mathbb{P}(M_1 \mid \mathbf X)} = \frac{\mathbb{P}(M_0)}{\mathbb{P}(M_1)} \frac{\mathbb{P}(\mathbf X \mid M_0)}{\mathbb{P}(\mathbf X \mid M_1)},
\end{aligned}</script>

<p>where $\mathbb{P}(\mathbf X \mid M_i)$ is the <em>marginal likelihood</em> of
the data (this may be familiar to you as the denominator of Bayes
Theorem, which is variously referred to as the <em>model evidence</em> or
<em>integrated likelihood</em>). Formally, the marginal likelihood is the
probability of the observed data ($\mathbf X$) under a given model
($M_i$) that is averaged over all possible values of the parameters of
the model ($\theta_i$) with respect to the prior density on $\theta_i$</p>

<script type="math/tex; mode=display">\begin{equation}
\mathbb{P}(\mathbf X \mid M_i) = \int \mathbb{P}(\mathbf X \mid \theta_i) \mathbb{P}(\theta_i)dt.
\tag{Marginal Likelihood}\label{eq:marginal_likelihood}
\end{equation}</script>

<p>This makes it clear that more complex (parameter-rich) models are
penalized by virtue of the associated prior: each additional parameter
entails integration of the likelihood over the corresponding prior
density. If you refer back to equation \eqref{eq:BF}, you can see that, with
very little algebra, the ratio of marginal likelihoods is equal to the
Bayes factor:</p>

<script type="math/tex; mode=display">\begin{equation}
BF(M_0,M_1) = \frac{\mathbb{P}(\mathbf X \mid M_0)}{\mathbb{P}(\mathbf X \mid M_1)} = \frac{\mathbb{P}(M_0 \mid \mathbf X, \theta_0)}{\mathbb{P}(M_1 \mid \mathbf X, \theta_1)} \div \frac{\mathbb{P}(M_0)}{\mathbb{P}(M_1)}. 
\label{eq:bf_Formula}
\end{equation}</script>

<p>Therefore, we can perform a Bayes factor comparison of two models by
calculating the marginal likelihood for each one. Alas, exact solutions
for calculating marginal likelihoods are not known for phylogenetic
models (see equation \eqref{eq:marginal_likelihood}), thus we must resort to numerical
integration methods to estimate or approximate these values. In this
exercise, we will estimate the marginal likelihood for each partition
scheme using both the stepping-stone <a href="#Xie2011">(Xie et al. 2011; Fan et al. 2011)</a> and path
sampling estimators <a href="#Lartillot2006">(Lartillot 2006; Baele et al. 2012)</a>.</p>

<h2 class="section" id="substitution-models">Substitution Models</h2>

<p>The models we use here are equivalent to the models described in the
previous exercise on substitution models (continuous time Markov
models). To specify the model please consult the previous exercise.
Specifically, you will need to specify the following substitution
models:</p>

<ul>
  <li>Jukes-Cantor (JC) substitution model <a href="#Jukes1969">(Jukes and Cantor 1969)</a></li>
  <li>Hasegawa-Kishino-Yano (HKY) substitution model <a href="#Hasegawa1985">(Hasegawa et al. 1985)</a></li>
  <li>General-Time-Reversible (GTR) substitution model <a href="#Tavare1986">(Tavaré 1986)</a></li>
  <li>Gamma (+G) model for among-site rate variation <a href="#Yang1994a">(Yang 1994)</a></li>
  <li>Invariable-sites (+I) model <a href="#Hasegawa1985">(Hasegawa et al. 1985)</a></li>
</ul>

<h2 class="section" id="estimating-the-marginal-likelihood">Estimating the Marginal Likelihood</h2>

<p>We will estimate the marginal likelihood of a given model using a
‘stepping-stone’ (or ‘path-sampling’) algorithm. These algorithms are
similar to the familiar MCMC algorithms, which are intended to sample
from (and estimate) the joint posterior probability of the model
parameters. Stepping-stone algorithms are like a series of MCMC
simulations that iteratively sample from a specified number of
distributions that are discrete steps between the posterior and the
prior probability distributions. The basic idea is to estimate the
probability of the data for all points between the posterior and the
prior—effectively summing the probability of the data over the prior
probability of the parameters to estimate the marginal likelihood.
Technically, the steps correspond to a series of <code class="highlighter-rouge">powerPosteriors()</code>,
where the likelihood is iteratively raised to a series of numbers
between 1 and 0 (Figure [fig:ss]). When the likelihood is raised to
the power of 1 (typically the first stepping stone), samples are drawn
from the (untransformed) posterior. By contrast, when the likelihood is
raised to the power of 0 (typically the last stepping stone), samples
are drawn from the prior. To perform a stepping-stone simulation, we
need to specify (1) the number of stepping stones (power posteriors)
that we will use to traverse the path between the posterior and the
prior (<em>e.g.</em>, we specify 50 or 100 stones),
(2) the spacing of the stones between the posterior and prior
(<em>e.g.</em>, we may specify that the stones are
distributed according to a beta distribution), (3) the number of samples
(and their thinning) to be drawn from each stepping stone, and (4) the
direction we will take (<em>i.e.</em>, from the
posterior to the prior or vice versa).</p>

<blockquote class="figure">
  <p><img src="figures/ss.png" alt="" /> 
Estimating marginal likelihoods using
stepping-stone simulation. Estimating the marginal likelihood involves
integrating the likelihood of the data over the entire prior probability
density for the model parameters.MCMC algorithms target the posterior
probability density, which is typically concentrated in a small region
of the prior probability density (A).Accordingly, standard MCMC
simulation cannot provide unbiased estimates of the marginal likelihood
because it will typically fail to explore most of the prior density.(B)
Stepping-stone algorithms estimate the marginal likelihood by means of a
series of MCMC-like simulations, where the likelihood is iteratively
raised to a series of powers, effectively forcing the simulation to more
fully explore the prior density of the model parameters.Here, six
uniformly spaced stones span the posterior, where the power posterior is
$\beta=6/6=1$, to the prior, where the power posterior is $\beta=0/6=0$.</p>
</blockquote>

<p>This method computes a vector of powers from a beta distribution, then
executes an MCMC run for each power step while raising the likelihood to
that power. In this implementation, the vector of powers starts with 1,
sampling the likelihood close to the posterior and incrementally
sampling closer and closer to the prior as the power decreases.</p>

<p>Just to be safe, it is better to clear the workspace (if you did not
just restart RevBayes):</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>clear()
</code></pre></div></div>

<p>Now set up the model as in the previous exercise. You should start with
the simple Jukes-Cantor substitution model. Setting up the model
requires:</p>

<ol>
  <li>Loading the data and retrieving useful variables about it
(<em>e.g.</em>, number of sequences and
taxon names).</li>
  <li>Specifying the instantaneous-rate matrix of the substitution model.</li>
  <li>Specifying the tree model including branch-length variables.</li>
  <li>Creating a random variable for the sequences that evolved under
the <code class="highlighter-rouge">PhyloCTMC</code>.</li>
  <li>Clamping the data.</li>
  <li>Creating a model object.</li>
  <li>Specifying the moves for parameter updates.</li>
</ol>

<p>The following procedure for estimating marginal likelihoods is valid for
any model in RevBayes. You will need to repeat this later for other
models. First, we create the variable containing the power-posterior
analysis. This requires that we provide a model and vector of moves, as
well as an output file name. The <code class="highlighter-rouge">cats</code> argument sets the number of
stepping stones.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    pow_p = powerPosterior(mymodel, moves, monitors, "output/model1.out", cats=50) 
</code></pre></div></div>
<p>We can start the power-posterior analysis by first burning in the chain
and and discarding the first 10000 states. This will help ensure that
analysis starts from a region of high posterior probability, rather than
from some random point.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    pow_p.burnin(generations=10000,tuningInterval=1000)
</code></pre></div></div>
<p>Now execute the run with the <code class="highlighter-rouge">.run()</code> function:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    pow_p.run(generations=1000)  
</code></pre></div></div>
<p>Once the power posteriors have been saved to file, create a stepping
stone sampler. This function can read any file of power posteriors and
compute the marginal likelihood using stepping-stone sampling.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ss = steppingStoneSampler(file="output/model1.out", powerColumnName="power", likelihoodColumnName="likelihood")
</code></pre></div></div>
<p>These commands will execute a stepping-stone simulation with 50 stepping
stones, sampling 1000 states from each step. Compute the marginal
likelihood under stepping-stone sampling using the member function
<code class="highlighter-rouge">marginal()</code> of the <code class="highlighter-rouge">ss</code> variable and record the value in Table
[tab:ml_cytb].</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ss.marginal() 
</code></pre></div></div>
<p>Path sampling is an alternative to stepping-stone sampling and also
takes the same power posteriors as input.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ps = pathSampler(file="output/model1.out", powerColumnName="power", likelihoodColumnName="likelihood")
</code></pre></div></div>
<p>Compute the marginal likelihood under stepping-stone sampling using the
member function <code class="highlighter-rouge">marginal()</code> of the <code class="highlighter-rouge">ps</code> variable and record the value
in Table [tab:ml_cytb].</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    ps.marginal() 
</code></pre></div></div>

<p>As an example we provide the file
<strong>RevBayes_scripts/marginalLikelihood_JukesCantor.Rev</strong>.</p>

<p>We have kept this description of how to use stepping-stone-sampling and
path-sampling very generic and did not provide the information about the
model here. Our main motivation is to show that the marginal likelihood
estimation algorithms are independent of the model. Thus, you can apply
these algorithms to any model, <em>e.g.</em>, relaxed
clock models and birth-death models, as well.</p>

<h2 class="subsection" id="exercises">Exercises</h2>

<ul>
  <li>Compute the marginal likelihoods of the <em>cytb</em> alignment for the
following substitution models:
    <ol>
      <li>Jukes-Cantor (JC) substitution model</li>
      <li>Hasegawa-Kishino-Yano (HKY) substitution model</li>
      <li>General-Time-Reversible (GTR) substitution model</li>
      <li>GTR with gamma distributed-rate model (GTR+G)</li>
      <li>GTR with invariable-sites model (GTR+I)</li>
      <li>GTR+I+G model</li>
    </ol>
  </li>
  <li>Enter the marginal likelihood estimate for each model in the
corresponding cell of Table [tab:ml_cytb].</li>
  <li>Which is the best fitting substitution model?</li>
</ul>

<figure id="tab_ml_subst_models"><table>
  <thead>
    <tr>
      <th style="text-align: right"><strong>Model</strong></th>
      <th style="text-align: center"><strong>Path-Sampling</strong></th>
      <th style="text-align: center"><strong>Stepping-Stone-Sampling</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">JC ($M_1$)</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: right">HKY ($M_2$)</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: right">GTR ($M_3$)</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: right">GTR+$\Gamma$ ($M_4$)</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: right">GTR+I ($M_5$)</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
    <tr>
      <td style="text-align: right">GTR+$\Gamma$+I ($M_6$)</td>
      <td style="text-align: center"> </td>
      <td style="text-align: center"> </td>
    </tr>
  </tbody>
</table>

<figcaption>Marginal likelihoods for different substitution models.</figcaption>
</figure>

<h1 class="section" id="computing-bayes-factors-and-model-selection">Computing Bayes Factors and Model Selection</h1>

<p>Now that we have estimates of the marginal likelihood for each of our
the candidate substitution models, we can evaluate their relative fit to
the datasets using Bayes factors. Phylogenetic programs log-transform
the likelihood values to avoid
<a href="http://en.wikipedia.org/wiki/Arithmetic_underflow">underflow</a>:
multiplying likelihoods (numbers $&lt; 1$) generates numbers that are too
small to be held in computer memory. Accordingly, we need to use a
different form of equation [bfFormula] to calculate the ln-Bayes
factor (we will denote this value $\mathcal{K}$):</p>

<script type="math/tex; mode=display">\begin{equation}
\mathcal{K}=\ln[BF(M_0,M_1)] = \ln[\mathbb{P}(\mathbf X \mid M_0)]-\ln[\mathbb{P}(\mathbf X \mid M_1)],
\label{LNbfFormula}
\end{equation}</script>

<p>where $\ln[\mathbb{P}(\mathbf X \mid M_0)]$ is the <em>marginal lnL</em>
estimate for model $M_0$. The value resulting from equation
[LNbfFormula] can be converted to a raw Bayes factor by simply taking
the exponent of $\cal{K}$</p>

<script type="math/tex; mode=display">\begin{equation}
BF(M_0,M_1) = e^{\cal{K}}.
\label{LNbfFormula2}
\end{equation}</script>

<p>Alternatively, you can directly interpret the strength of evidence in favor of $M_0$ in log
space by comparing the values of $\cal{K}$ to the appropriate scale
(Table [bftable], second column). In this case, we evaluate $\cal{K}$
in favor of model $M_0$ against model $M_1$ so that:</p>

<blockquote>
  <p>if $\mathcal{K} &gt; 1$, model $M_0$ is preferred<br />
if $\mathcal{K} &lt; -1$, model $M_1$ is preferred.</p>
</blockquote>

<p>Thus, values of $\mathcal{K}$ around 0 indicate that there is no
preference for either model.</p>

<p>Using the values you entered in Table [tab:ml_cytb] and equation
[LNbfFormula], calculate the ln-Bayes factors (using $\mathcal{K}$)
for each model comparison. Enter your answers in Table [bfTable2]
using the stepping-stone and the path-sampling estimates of the marginal
log-likelihoods.</p>

<p>[bfTable2]</p>

<h1 class="section" id="for-your-consideration">For your consideration…</h1>

<p>In this tutorial you have learned how to use RevBayes to assess the
<em>relative</em> fit of a pool of candidate substitution models to a given
sequence alignment. Typically, once we have identified the “best”
substitution model for our alignment, we would then proceed to use this
model for inference. Technically, this is a decision to condition our
inferences on the selected model, which explicitly assumes that it
provides a reasonable description of the process that gave rise to our
data. However, there are several additional issues to consider before
proceeding along these lines, which we briefly mention below.</p>

<h2 class="subsection" id="accommodating-model-uncertainty">Accommodating Model Uncertainty</h2>

<p>In some or many situations the number of possible models to compare is
large, <em>e.g.</em>, choosing all possible
combinations of substitution models <a href="#Huelsenbeck2004">(Huelsenbeck et al. 2004)</a>. Furthermore,
imagine, for example, that there are several (possibly many) alternative
models that provide a similarly good fit to our given dataset. In such
scenarios, conditioning inference on <em>any</em> single model (even the
‘best’) ignores uncertainty in the chosen model, which can cause
estimates to be biased. This is the issue of <em>model uncertainty</em>. The
Bayesian framework provides a natural approach for accommodating model
uncertainty by means of <em>model averaging</em>; we simply adopt the
perspective that models (like standard parameters) are random variables,
and integrate the inference over the distribution of candidate models.
We will demonstrate how to accommodate model uncertainty using
RevBayes in a separate tutorial, RB_ModelAveraging_Tutorial.</p>

<h2 class="subsection" id="assessing-model-adequacy">Assessing Model Adequacy</h2>

<p>In this tutorial, we used Bayes factors to assess the fit of various
substitution models to our sequence data, effectively establishing the
<em>relative</em> rank of the candidate models. Even if we have successfully
identified the very best model from the pool of candidates, however, the
preferred model may nevertheless be woefully inadequate in an <em>absolute</em>
sense. For this reason, it is important to consider <em>model adequacy</em>:
whether a given model provides a reasonable description of the process
that gave rise to our sequence data. We can assess the absolute fit of a
model to a given dataset using <em>posterior predictive simulation</em>. This
approach is based on the following premise: if the candidate model
provides a reasonable description of the process that gave rise to our
dataset, then we should be able to generate data under this model that
resemble our observed data. We will demonstrate how to assess model
adequacy using RevBayes in a separate tutorial,
RB_ModelAdequacy_Tutorial.</p>


<ol class="bibliography"><li><span id="Hoehna2017a">Höhna S., Landis M.J., Heath T.A. 2017. Phylogenetic Inference using RevBayes. Current Protocols in Bioinformatics.</span>

<a href="https://doi.org/10.1002/cpbi.22">10.1002/cpbi.22</a>

</li>
<li><span id="Suchard2001">Suchard M.A., Weiss R.E., Sinsheimer J.S. 2001. Bayesian Selection of Continuous-Time Markov Chain Evolutionary Models. Molecular Biology and Evolution. 18:1001–1013.</span>

</li>
<li><span id="Lartillot2006">Lartillot N. 2006. Conjugate Gibbs Sampling for Bayesian Phylogenetic Models. Journal of Computational Biology. 13:1701–1722.</span>

</li>
<li><span id="Xie2011">Xie W., Lewis P.O., Fan Y., Kuo L., Chen M.H. 2011. Improving Marginal Likelihood Estimation for Bayesian Phylogenetic Model Selection. Systematic Biology. 60:150–160.</span>

</li>
<li><span id="Baele2012">Baele G., Lemey P., Bedford T., Rambaut A., Suchard M.A., Alekseyenko A.V. 2012. Improving the Accuracy of Demographic and Molecular Clock Model Comparison while Accommodating Phylogenetic Uncertainty. Molecular Biology and Evolution. 29:2157–2167.</span>

<a href="https://doi.org/10.1093/molbev/mss084">10.1093/molbev/mss084</a>

</li>
<li><span id="Baele2013">Baele G., Li W.L.S., Drummond A.J., Suchard M.A., Lemey P. 2013. Accurate Model Selection of Relaxed Molecular Clocks in Bayesian Phylogenetics. Molecular Biology and Evolution. 30:239–243.</span>

<a href="https://doi.org/10.1093/molbev/mss243">10.1093/molbev/mss243</a>

</li>
<li><span id="Lavine1999">Lavine M., Schervish M.J. 1999. Bayes Factors: What They Are and What They Are Not. The American Statistician. 53:119–122.</span>

<a href="https://doi.org/10.1080/00031305.1999.10474443">10.1080/00031305.1999.10474443</a>

</li>
<li><span id="Jeffreys1961">Jeffreys H. 1961. The Theory of Probability. Oxford University Press.</span>

<a href="https://doi.org/10.1038/109132a0">10.1038/109132a0</a>

</li>
<li><span id="Fan2011">Fan Y., Wu R., Chen M.-H., Kuo L., Lewis P.O. 2011. Choosing among Partition Models in Bayesian Phylogenetics. Molecular Biology and Evolution. 28:523–532.</span>

</li>
<li><span id="Jukes1969">Jukes T.H., Cantor C.R. 1969. Evolution of Protein Molecules. Mammalian Protein Metabolism. 3:21–132.</span>

<a href="https://doi.org/10.1016/B978-1-4832-3211-9.50009-7">10.1016/B978-1-4832-3211-9.50009-7</a>

</li>
<li><span id="Hasegawa1985">Hasegawa M., Kishino H., Yano T. 1985. Dating of the Human-Ape Splitting by a molecular Clock of Mitochondrial DNA. Journal of Molecular Evolution. 22:160–174.</span>

<a href="https://doi.org/10.1007/BF02101694">10.1007/BF02101694</a>

</li>
<li><span id="Tavare1986">Tavaré S. 1986. Some Probabilistic and Statistical Problems in the Analysis of DNA Sequences. Some Mathematical Questions in Biology: DNA Sequence Analysis. 17:57–86.</span>

</li>
<li><span id="Yang1994a">Yang Z. 1994. Maximum Likelihood Phylogenetic Estimation from DNA Sequences with Variable Rates Over Sites: Approximate Methods. Journal of Molecular Evolution. 39:306–314.</span>

<a href="https://doi.org/10.1007/BF00160154">10.1007/BF00160154</a>

</li>
<li><span id="Huelsenbeck2004">Huelsenbeck J.P., Larget B., Alfaro M.E. 2004. Bayesian Phylogenetic Model Selection using Reversible Jump Markov Chain Monte Carlo. Molecular Biology and Evolution. 21:1123.</span>

</li>
<li><span id="Hoehna2016b">Höhna S., Landis M.J., Heath T.A., Boussau B., Lartillot N., Moore B.R., Huelsenbeck J.P., Ronquist F. 2016. RevBayes: Bayesian Phylogenetic Inference Using Graphical Models and an Interactive Model-Specification Language. Systematic Biology. 65:726–736.</span>

<a href="https://doi.org/10.1093/sysbio/syw021">10.1093/sysbio/syw021</a>

</li></ol>

<script type="text/javascript">
var _ol = document.querySelectorAll('ol');
for (var i = 0, elem_ol; elem_ol = _ol[i]; i++) {
	if ( elem_ol.classList == "bibliography" ) {
		var _li = elem_ol.getElementsByTagName("li");
		//for (var j = 0, elem_li; elem_li = _li[j]; j++)
		//{
		//	elem_li.innerHTML = elem_li.innerHTML.replace(/(https?:\/\/)([^\s<]+)/,"<a href=\"$1$2\">$2");
		//}
		if(_li.length > 0)
			elem_ol.outerHTML = "<h2>References</h2>"+elem_ol.outerHTML
	}
}
</script>

      <br>
<footer>
  <div class="container">
  <div class="row">
    <div class="col-sm-12" align="center">
      <a href="https://github.com/revbayes">GitHub</a> | <a href="/revbayes-site/license">License</a> | <a href="/revbayes-site/citation">Citation</a> | <a href="https://groups.google.com/forum/#!forum/revbayes-users">Users Forum</a>
    </div>
  </div>
  <br>
  </div>
</footer>

    </div>
    <script src="/revbayes-site/assets/js/vendor/jquery.min.js"></script>
<script src="/revbayes-site/assets/js/vendor/FileSaver.min.js"></script>
<script src="/revbayes-site/assets/js/vendor/jszip.min.js"></script>
<script src="/revbayes-site/assets/js/vendor/bootstrap.min.js"></script>

<script type="text/javascript">
// Add default language
$(":not(code).highlighter-rouge").each(function() {
  
  if( this.classList == "highlighter-rouge") {
    this.classList = "Rev highlighter-rouge";
  }
  
});
// $("code.highlighter-rouge").each(function() {
//   
//   if( this.classList == "highlighter-rouge") {
//       this.classList = "Rev highlighter-rouge";
//   }
//   
// });
</script>

<script src="/revbayes-site/assets/js/base.js"></script>

<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>

  </body>
</html>
