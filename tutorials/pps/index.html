<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="search-domain" value="https://willpett.github.io/revbayes_tutorials/">
    <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet">
    <link rel="stylesheet" href="/revbayes_tutorials/assets/css/syntax.css">
    <link rel="stylesheet" type="text/css" href="/revbayes_tutorials/assets/css/bootstrap.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes_tutorials/assets/css/bootstrap-theme.css" />
    <link rel="stylesheet" type="text/css" href="/revbayes_tutorials/assets/css/main.css" />
    <title>RevBayes: Assessing Phylogenetic Reliability Using ‘RevBayes‘ and $P^{3}$</title>
  </head>
  <body>
    <div class="container">
      <nav class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>

      <a href="https://revbayes.github.io" class="pull-left">
        <img class="navbar-logo" src="/revbayes_tutorials/assets/img/aquabayes-desaturated.png" alt="RevBayes Logo" />
      </a>
      
      
      <a class="navbar-brand" href="/revbayes_tutorials/">Home</a>

    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
      <ul class="nav navbar-nav">

	
        <li><a href="/revbayes_tutorials/software/">Software</a></li>
        <li><a href="/revbayes_tutorials/developer/">Developer</a></li>
        <li><a href="/revbayes_tutorials/tutorials/">Tutorials</a></li>
        <li><a href="/revbayes_tutorials/workshops/">Workshops</a></li>
      </ul>
      <form class="navbar-form navbar-right" role="search" id="search" onsubmit="google_search(); return false;">
        <div class="form-group">
          <input type="text" id="google-search" placeholder="Search..." aria-label="Google site search">
        </div>
      </form>
    </div>
  </div>
</nav>

      <div class="row">
	<h1 class="maintitle">Assessing Phylogenetic Reliability Using ‘RevBayes‘ and $P^{3}$</h1>
	<h3 class="subtitle">Model adequacy testing using posterior prediction</h3>
	<h4 class="maintitle">Lyndon M. Coghill, Will Freyman, Sebastian H&#246;hna and Jeremy M. Brown</h4>
</div>

      
<blockquote class="overview no-print" id="overview">
  <h2>Overview</h2>
  <div class="row">
    <div class="col-md-9">
        <strong>Prerequisites</strong>
        
          <ul id="prerequisites">
          
            
            
            <li><a href="/revbayes_tutorials/tutorials/intro/">Statistical Inference Using RevBayes</a></li>
          
          </ul>
        
    </div>
  </div>
</blockquote>

<blockquote class="tutorial_files no-print" id="tutorial_files">
  <h2>Data files and scripts</h2>
  
    <div class="row" id="script_row">
      <div class="col-md-9">
          <strong>Scripts</strong>
          <ul id="scripts"></ul>
        </div>
    </div>
</blockquote>
      <h1 id="overview">Overview</h1>

<p>This tutorial presents the general principles of assessing the
reliability of a phylogenetic inference through the use of posterior
predictive simulation (PPS). PPS works by assessing the fit of an
evolutionary model to a given dataset, and analyzing several test
statistics using a traditional goodness-of-fit framework to help explain
where a model may be the most inadequate.</p>

<h2 id="requirements">Requirements</h2>

<p>We assume that you have read and hopefully completed the following
tutorials:</p>

<ul>
  <li>
    <p>RB_Getting_Started</p>
  </li>
  <li>
    <p>RB_Data_Tutorial</p>
  </li>
  <li>
    <p>RB_MCMC_Tutorial</p>
  </li>
</ul>

<p>This means that we will assume that you know how to execute and load
data into ‘RevBayes‘, are familiar with some basic commands, and know
how to perform an analysis of a single-gene dataset (assuming an
unconstrained/unrooted tree).</p>

<h1 id="data-and-files">Data and files</h1>

<p>We provide the necessary data and scripts that we will use in this
tutorial. Of course, you may want to use your own dataset instead. In
the ‘data‘ folder, you will find the following files</p>

<ul>
  <li><strong>data/primates_and_galeopterus_cytb.nex</strong>: A simple dataset
consisting of 8 taxa with 500 characters simulated under the
GTR model.</li>
</ul>

<h1 id="introduction">Introduction</h1>

<p>Assessing the fit of an evolutionary model to the data is critical as
using a model with poor fit can lead to spurious conclusions. However, a
critical evaluation of absolute model fit is rare in evolutionary
studies. Posterior prediction is a Bayesian approach to assess the fit
of a model to a given dataset
(missing reference), that relies on the
use of the posterior and the posterior predictive distributions. The
posterior distribution is the standard output from Bayeisan phylogenetic
inference. The posterior predictive distribution represents a range of
possible outcomes given the assumptions of the model. The most common
method to generate these possible outcomes, is to sample parameters from
the posterior distribution, and use them to simulate new replicate
datasets (Fig. 1). If these simulated datasets differ from the empirical
dataset in a meaningful way, the model is failing to capture some
salient feature of the evolutionary process.</p>

<blockquote class="figure">
  <p><img src="figures/brown_2014.png" alt="" /> 
A schematic presentation of data- versus
inference-based approaches to assessing model plausibility with
posterior predictivesimulation. Most statistics proposed for testing
model plausibility compare data-based characteristics of the original
data set to the posterior predictive data sets (e.g., variation in
GC-content across species). ‘RevBayes‘ additionally implements test
statistics that compare the inferences resulting from different data
sets (e.g., the distribution of posterior probability across
topologies). Multiple sequence alignments (MSAs) are represented as
shaded matrices and arrows originating from MSAs point to the MCMC
samples of tree topologies and scalar model parameters ($\theta$)
resulting from Bayesian analysis of that MSA. Subscripts of MCMC samples
taken during analysis of the original data index the samples
$(1,    …, n)$. Subscripts for each posterior predictive data set
indicate which MCMC sample was used in its simulation. Subscripts for
MCMC samples resulting from analysis of a posterior predictive data set
first indicate the posterior predictive data set that was analyzed and
next index the MCMC samples from analysis of that particular data set
$(1, …, m)$.</p>
</blockquote>

<p>The framework to construct posterior predictive distributions, and
compare them to the posterior distribution is conveniently built in to
‘RevBayes‘. In this tutorial we will walk you through using this
functionality to perform a complete posterior predictive simulation on
an example dataset.</p>

<h2 id="data--versus-inference-based-comparisons">Data- Versus Inference-Based Comparisons</h2>

<p>Most statistics proposed for testing model plausibility compare
data-based characteristics of the original data set to the posterior
predictive data sets (e.g., variation in GC-content across species). In
data-based assessments of model fit one compares the empirical data to
data simulated from samples of the posterior distribution. ‘RevBayes‘
additionally implements test statistics that compare the inferences
resulting from different data sets (e.g., the distribution of posterior
probability across topologies). These are called inference-based
assessments of model fit. For these assessments one must run an MCMC
analysis on each simulated data set, and then compare the inferences
made from the simulated data to the inference made from the empirical
data.</p>

<p>Due to time constraints, in today’s tutorial we will only cover the
data-based method of assessing model plausibility. The inference-based
method can be a powerful tool that you may want to explore at another
time.</p>

<h2 id="secUnif">Substitution Models</h2>

<p>The models we use here are equivalent to the models described in the
previous exercise on substitution models (continuous time Markov
models). To specify the model please consult the previous exercise.
Specifically, you will need to specify the following substitution
models:</p>

<ul>
  <li>
    <p>Jukes-Cantor (JC) substitution model (missing reference)</p>
  </li>
  <li>
    <p>General-Time-Reversible (GTR) substitution model (missing reference)</p>
  </li>
  <li>
    <p>Gamma (+G) model for among-site rate variation (missing reference)</p>
  </li>
  <li>
    <p>Invariable-sites (+I) model (missing reference)</p>
  </li>
</ul>

<h1 id="assessing-model-fit-with-posterior-prediction">Assessing Model Fit with Posterior Prediction</h1>

<p>The entire process of posterior prediction can be executed by using the
<strong>data_pp_analysis_JC.Rev</strong> script in the <strong>scripts</strong> folder. If you
were to type the following command into ‘RevBayes‘:</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("scripts/data_pp_analysis_JC.Rev")
</code></pre></div></div>

<p>the entire data-based posterior prediction process would run on the
example dataset. However, in this tutorial, we will walk through each
step of this process on an example dataset.</p>

<h2 id="empirical-mcmc-analysis">Empirical MCMC Analysis</h2>

<p>To begin, we first need to generate a posterior distribution from which
to sample for simulation. This is the normal, and often only, step
conducted in phylogenetic studies. Here we will specify our dataset,
evolutionary model, and run a traditional MCMC analysis.</p>

<p>This code is all in the <strong>data_pp_analysis_JC.Rev</strong> file.</p>

<h3 id="set-up-the-workspace">Set up the workspace</h3>

<p>First, let’s set up some workspace variables we’ll need. We’ll specify a
general name to apply to your analysis. This will be used for future
output files, so make sure it’s something clear and easy to understand.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>analysis_name = "pps_example"
model_name = "JC"
model_file_name = "scripts/"+model_name+"_Model.Rev"
</code></pre></div></div>

<p>Now specify and read in the input file. This is your sequence alignment
in NEXUS format.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>in_file = "data/primates_and_galeopterus_cytb.nex"
data &lt;- readDiscreteCharacterData(in_file)
</code></pre></div></div>

<h3 id="specify-the-model">Specify the model</h3>

<p>Now we’ll call a separate script <strong>JC_Model.Rev</strong> that specifies the JC
model:</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source( model_file_name ) 
</code></pre></div></div>

<p>If we open the <strong>JC_Model.Rev</strong> script, most of this script should look
familiar from the RB_MCMC_Tutorial. There are a couple of specific
lines we can look at that might be a little different though, as we are
using a unrooted tree for this analysis.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Q &lt;-fnJC(4)
</code></pre></div></div>

<p>Here we are specifying that we should use the Jukes-Cantor model, and
have it applied uniformly to all sites in the dataset. While this
obviously is not likely to be a good fitting model for most datasets, we
are using it for simplicity of illustrating the process.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>topology ~ dnUniformTopology(names)
</code></pre></div></div>

<p>This sets a uniform prior on the tree topology.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>br_lens[i] ~ dnExponential(10.0)
</code></pre></div></div>

<p>This sets an exponential distribution as the branch length prior.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>phylogeny := treeAssembly(topology, br_lens)
</code></pre></div></div>

<p>This builds the tree by combining the topology with branch length
support values.</p>

<h3 id="run-the-mcmc">Run the MCMC</h3>

<p>Now let’s run MCMC on our empirical dataset, just like a normal
phylogenetic analysis. Here we use the Jukes-Cantor substition model.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mcmc_gen = 10000
burnin_gen = 2000
source("scripts/MCMC_Empirical.Rev")
</code></pre></div></div>

<p>After specifying a model, we will conduct the MCMC analysis with the
script <strong>MCMC_Empirical.Rev</strong>. Much of that script should look familiar
from the <strong>RB_CTMC_Tutorial</strong>, but let’s take a look at the script and
break down and revisit some of the major pieces as a refresher. Much of
this will be a familiar to you from the <strong>RB_CTMC_Tutorial</strong>.</p>

<p>First we need to specify some monitors.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mni = 0
monitors[++mni] = mnModel(filename="output" + n + "/" + analysis_name + "_posterior.log",printgen=2500, separator = TAB)
</code></pre></div></div>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>monitors[++mni] = mnFile(filename="output" + n + "/" + analysis_name + "_posterior.trees",printgen=2500, separator = TAB, phylogeny)
</code></pre></div></div>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>monitors[++mni] = mnScreen(printgen=2500, TL)
</code></pre></div></div>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>monitors[++mni] = mnStochasticVariable(filename="output" + n + "/" + analysis_name + "_posterior.var",printgen=2500)
</code></pre></div></div>

<p>Here the monitors are just being advanced by the value of mni which is
being incremented each generation.</p>

<p>Next, we will call the MCMC function, passing it the model, monitors and
moves we specified above to set to build our mymcmc object.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymcmc = mcmc(mymodel, monitors, moves, nruns=2)
</code></pre></div></div>

<p>Specify a burnin for this run.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymcmc.burnin(generations=burnin_gen,tuningInterval=100)
</code></pre></div></div>

<p>Finally, we will execute the run for a specified number of generations.
Here we are only using a small number of generations for the tutorial,
however with empirical data you will most likely need a much larger
number of generations to get a well mixed sample. The number of
generations and printed generations is important to consider here for a
variety of reasons, in particular for posterior predictive simulation.
When we simulate datasets in the next step, we can only simulate 1
dataset per sample in our posterior. So, while the number of posterior
samples will almost always be larger than the number of datasets we will
want to simulate, it’s something to keep in mind.</p>

<p>Now let’s actually run the MCMC in ‘RevBayes‘. This process should take
approximately 15-20 minutes depending on the speed of your computer.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mymcmc.run(generations=mcmc_gen)
</code></pre></div></div>

<h3 id="mcmc-output">MCMC output</h3>

<p>After the process completes, the results can be found in the
<strong>output_JC</strong> folder. You should see a number of familiar looking
files, all with the name we provided under the <strong>analysis_name</strong>
variable, <strong>pps_example</strong> in this case. Since we set the number of runs
(nruns=2) in our MCMC, there will be two files of each type (.log .trees
.var) with an _<em>N</em> where <em>N</em> is the run number. You will also see 3
files without any number in their name. These are the combined files of
the output. These will be the files we use for the rest of the process.
If you open up one of the combined .var file, you should see that there
are 200 samples. This was produced by our number of generations
(250,000) divided by our number of printed generations, (2500) what we
specified earlier. This is important to note, as we will need to thin
these samples appropriately in the next step to get the proper number of
simulated datasets.</p>

<h2 id="posterior-predictive-data-simulation">Posterior Predictive Data Simulation</h2>

<p>The next step of posterior predictive simulation is to simulate new
datasets by drawing samples and parameters from the posterior
distribution generated from the empirical MCMC anlaysis.</p>

<p>In the <strong>data_pp_analysis_JC.Rev</strong> script, that is conducted using
the following lines of RevScript:</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>source("scripts/PosteriorPredictive_Simulation.Rev")
</code></pre></div></div>

<p>Let’s take a look at each of the calls in the
<strong>PosteriorPredictive_Simulation.Rev</strong> script so that we can understand
what is occurring.</p>

<p>First, we read in the trace file of the Posterior Distribution of
Variables.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>trace = readStochasticVariableTrace("output" + "/" + analysis_name + "_posterior.var", delimiter=TAB)
</code></pre></div></div>

<p>Now we call the <strong>posteriorPredictiveSimulation()</strong> function, which
accepts any valid model of sequence evolution, and output directory, and
a trace. For each line in the trace, it will simulate a new dataset
under the specified model.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pps = posteriorPredictiveSimulation(mymodel, directory="output" + "/" + analysis_name + "_post_sims", trace)
</code></pre></div></div>

<p>Now we run the posterior predictive simulation, generating a new dataset
for each line in the trace file that was read in. This is the part where
we need to decide how many simulated datasets we want to generate. If we
just use the pps.run() command, one dataset will be generated for each
sample in our posterior distribution. In this case, since we are reading
in the combined posterior trace file with 200 samples, it would generate
200 simulated datasets. If you want to generate fewer, say 100 datasets,
you need to use the thinning argument as above. In this case, we are
thinning the output by 2, that is we are dividing our number of samples
by 2. So that in our example case, we will end up simulating 100 new
datasets.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pps.run(thinning=2)
</code></pre></div></div>

<p>This process should finish in just a minute or two. If you look in the
<strong>output_JC</strong> folder, there should be another folder called
<strong>pps_example_post_sims</strong>. This folder is where the simulated
datasets are saved. If you open it up, you should see 100 folders named
posterior_predictive_sim_<em>N</em>. Where <em>N</em> is the number of the
simulated dataset. In each of these folders, you should find a seq.nex
file. If you open one of those files, you’ll see it’s just a basic NEXUS
file. These will be the datasets we analyze in the next step.</p>

<h2 id="calculating-the-test-statistics">Calculating the Test Statistics</h2>

<p>Now we will calculate the test statistics from the empirical data and
the simulated data sets. The part in the <strong>data_pp_analysis_JC.Rev</strong>
script that generates the test statistics is the following line:</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>num_post_sims = listFiles(path="output_"+model_name+"/" + analysis_name + "_post_sims").size()
source("scripts/PosteriorPredictive_DataSummary.Rev")
</code></pre></div></div>

<p>We will look at the major concepts of the
<strong>PosteriorPredictive_DataSummary.Rev</strong> to better understand how it
works. For a more complete discussion of the statistics involved, please
review (missing reference). In general, this script and these
statistics work by calculating the statistics of interest across each
posterior distribution from the simulated datasets, and comparing those
values to the values from the empirical posterior distribution.</p>

<p>The current version of this script generates over 30 summary statistics
including:</p>

<ul>
  <li>
    <p>Number Invariant Sites</p>
  </li>
  <li>
    <p>Max Pairwise Difference</p>
  </li>
  <li>
    <p>Max GC Content</p>
  </li>
  <li>
    <p>Min GC Content</p>
  </li>
  <li>
    <p>Mean GC Content</p>
  </li>
</ul>

<p>There are functions built-in to ‘RevBayes‘ to calculate these values for
you. Here are some examples from the
<strong>PosteriorPredictive_DataSummary.Rev</strong> script:</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>max_pd          = sim_data.maxPairwiseDifference( excludeAmbiguous=FALSE )
mean_gc_1       = sim_data.meanGcContentByCodonPosition(1, excludeAmbiguous=FALSE )
var_gc_1        = sim_data.varGcContentByCodonPosition(1, excludeAmbiguous=FALSE )
</code></pre></div></div>

<p>These same statistics are calculated for both the posterior
distributions from the simulated datasets and the posterior distribution
from the empirical dataset.</p>

<h2 id="calculating-p-values-and-effect-sizes">Calculating <em>P</em>-values and effect sizes</h2>

<p>Once we have the test statistics calculated for the simulated and
empirical posterior distributions, we can compare the simulated to the
empirical to get a goodness-of-fit. One simple way to do this is to
calculate a posterior predictive <em>P</em>value for each of the test
statistics of interest. This is done in the
<strong>data_pp_analysis_JC.Rev</strong> with the following lines :</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>emp_pps_file = "results_" + model_name + "/empirical_data_" + analysis_name + ".tsv"
sim_pps_file = "results_" + model_name + "/simulated_data_" + analysis_name + ".tsv"
outfileName = "results_" + model_name + "/data_pvalues_effectsizes_" + analysis_name + ".tsv"
source("scripts/PosteriorPredictive_PValues.Rev")
</code></pre></div></div>

<p>The 3 posterior predictive <em>P</em>values currently calculated by ‘RevBayes‘
are:</p>

<ul>
  <li>
    <p>Lower 1-tailed</p>
  </li>
  <li>
    <p>Upper 1-tailed</p>
  </li>
  <li>
    <p>2-tailed</p>
  </li>
</ul>

<p>The posterior predictive <em>P</em>value for a lower one-tailed test is the
proportion of samples in the distribution where the value is less than
or equal to the observed value, calculated as:</p>

<script type="math/tex; mode=display">p_l=p(T(X_{rep})\leqslant T(X)|(X)</script>

<p>The posterior predictive <em>P</em>value for a lower one-tailed test is the
proportion of samples in the distribution where the value is greater
than or equal to the observed value, calculated as:</p>

<script type="math/tex; mode=display">p_u=p(T(X_{rep})\geqslant T(X)|(X)</script>

<p>and the two-tailed posterior predictive <em>P</em>value is simply twice the
minimum of the corresponding one-tailed tests calculated as:</p>

<script type="math/tex; mode=display">p_2=2min(p_l,p_u)</script>

<p>Let’s take a look at the <strong>PosteriorPredictive_PValues.Rev</strong> script to
get a better idea of what is happening.</p>

<p>Using the equations outlined above, this script reads in the simulated
and empirical data we just calculated, and calls a few simple functions
for each of the test statistics:</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## Calculate and return a vector of lower, equal, and upper pvalues for a given test statistic
p_values &lt;- posteriorPredictiveProbability(numbers, empValue)

## 1-tailed
lower_p_value &lt;- p_values[1] 
equal_p_value &lt;- p_values[2] 
upper_p_value &lt;- p_values[3] 

## mid-point
midpoint_p_value = lower_p_value + 0.5*equal_p_value

## 2-tailed
two_tail_p_value = 2 * (min(v(lower_p_value, upper_p_value)))
</code></pre></div></div>

<p>these snippets calculate the various p-values of interest to serve as
our goodness-of-fit test.</p>

<p>Another way that you can calculate the magnitude of the discrepancy
between the empirical and PP datasets is by calculating the effect size
of each test statistic. Effect sizes are useful in quantifying the
magnitude of the difference between the empirical value and the
distribution of posterior predictive values. The test statistic effect
size can be calculated by taking the absolute value of the difference
between the median posterior predictive value and the empirical value
divided by the standard deviation of the posterior predictive
distribution (missing reference). Effect sizes are calculated automatically
for the inference based test statistics in the P analysis. The effect
sizes for each test statistics are stored in the same output file as the
<em>P</em>values.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>effect_size = abs((m - empValue) / stdev(numbers))
</code></pre></div></div>

<p>calculates the effect size of a given test statistic.</p>

<h2 id="viewing-the-results">Viewing the Results</h2>

<p>Once execution of the script is complete, you should see a new
directory, names <strong>results_JC</strong>. In this folder there should be 3
files. Each of these is a simple tab-delimited (TSV) file containing the
test statistic calculation output.</p>

<ul>
  <li>
    <p>empirical_pps_example.tsv</p>
  </li>
  <li>
    <p>pps_example.tsv</p>
  </li>
  <li>
    <p>pvalues_pps_example.tsv</p>
  </li>
</ul>

<p>If you have these 3 files, and there are results in them, you can go
ahead and quit ‘RevBayes‘.</p>

<p>[184,207,236]</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>q()
</code></pre></div></div>

<blockquote class="figure">
  <p><img src="figures/mean_gc.png" alt="" /> 
The distribution of mean GC values calculated
from the simulated data setsis shown. The dotted line represents the
mean GC calculated from the empirical data set.The Jukes-Cantor model
does not adequately describe the empirical data. This plot was generated
in <code class="highlighter-rouge">R</code>using the <strong>scripts/plot_results.R</strong> script.</p>
</blockquote>

<p>You can get an estimate of how the model performed by examining the
<em>P</em>values in the <strong>inference_pvalues_effectsizes_pps_example.csv</strong>
and <strong>data_pvalues_effectsizes_pps_example.csv</strong> files. In this
example, a quick view shows us that most of the statistics show a value
less than 0.05. This leads to a rejection of the model, suggesting that
the model employed is not a good fit for the data. This is the result we
expected given we chose a very simplistic model (JC) that would likely
be a poor fit for our data. However, it’s a good example of the
sensitivity of this method, showing how relatively short runtimes and a
low number of generations will still detect poor fit.</p>

<p>You can also use <code class="highlighter-rouge">R</code>to plot the results. Run the
<code class="highlighter-rouge">R</code>script <strong>scripts/plot_results.R</strong>. This will generate a
pdf plot for every test statistic.</p>

<h2 id="additional-exercises">Additional Exercises</h2>

<p>Included in the <strong>scripts</strong> folder is a second model script called
<strong>GTR_Model.Rev</strong>. As a personal exercise and a good test case, take
some time now, and run the same analysis, substituting the
<strong>GTR_Model.Rev</strong> model script for the <strong>JC_Model.Rev</strong> script we used
in the earlier example. You should get different results, this is an
excellent chance to explore the results and think about what they
suggest about the fit of the specified model to the dataset.</p>

<h3 id="additional-exercises-1">Additional Exercises</h3>

<p><strong>Some Questions to Keep in Mind:</strong></p>

<ul>
  <li>
    <p>Do you find the goodness-of-fit results to suggest that the GTR or
JC model is a better fit for our data?</p>
  </li>
  <li>
    <p>Which test statistics seem to show the strongest effect from the use
of a poorly fitting model?</p>
  </li>
  <li>
    <p>Other than <em>P</em>values, what other ways might you explore the test
statistic distributions to identify poor fit?</p>
  </li>
</ul>

<h1 id="for-your-consideration">For your consideration…</h1>

<p>In this tutorial you have learned how to use ‘RevBayes‘ to assess the
fit of a substitution model to a given sequence alignment. As you have
discovered, the observed data should be plausible under the posterior
predictive simulation if the model is reasonable. In phylogenetic
analyses we choose a model, which explicitly assumes that it provides an
reasonable explanation of the evolutionary process that generated our
data. However, just because a model may be the ’best’ model available,
does not mean it is an appropriate model for the data. This distinction
becomes both more critical and less obvious in modern analyses, where
the number of genes often number in the thousands. Posterior predictive
simulation in ‘RevBayes‘, allows you to easily check model fit for a
large number of genes by using global summaries to check the posterior
predictive distributions with a comfortable goodness-of-fit style
framework.</p>

<h2 id="batch-processing-of-large-datasets">Batch Processing of Large Datasets</h2>

<p>The process described above is for a single gene or alignment. However,
batch processing a large number of genes with this method is a
relatively straight forward process.</p>

<p>‘RevBayes‘ has built in support for MPI so running ‘RevBayes‘ on more
than a single processor, or on a cluster is as easy as calling it with
openmpi.</p>

<p>For example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mpirun -np 16 rb-mpi scripts/full_analysis.Rev
</code></pre></div></div>

<p>would run the entire posterior predictive simulation analysis on a
single dataset using 16 processors instead of a single processor. Use of
the MPI version of ‘RevBayes‘ will speed up the process dramatically.</p>

<p>Setting up the <strong>full_analysis.Rev</strong> script to cycle through a large
number of alignments is relatively simple as well. One easy way is to
provide a list of the data file names, and to loop through them. As an
example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data_file_list = "data_file_list.txt" 
data_file_list &lt;- readDiscreteCharacterData(data_file_list)
file_count = 0

for (n in 1:data_file_list.size()) {

FULL_ANALYSIS SCRIPT GOES HERE

file_count = file_count + 1
}
</code></pre></div></div>

<p>Then, anywhere in the full_analysis portion of the script that the
lines</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inFile = "data/8taxa_500chars_GTR.nex"
analysis_name = "pps_example"
</code></pre></div></div>

<p>appear, you would replace them with something along the lines of:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>inFile = n
analysis_name = "pps_example_" + file_count
</code></pre></div></div>

<p>This should loop through all of the data files in the list provided, and
run the full posterior predictive simulation analysis on each file.
Using a method like this, and combining it with the MPI call above, you
can scale this process up to multiple genes and spread the computational
time across several cores to speed it up.</p>

<p>Version dated:</p>


      <ol class="bibliography"></ol>

<script type="text/javascript">
var _ol = document.querySelectorAll('ol');
for (var i = 0, elem_ol; elem_ol = _ol[i]; i++) {
	if ( elem_ol.classList == "bibliography" ) {
		var _li = elem_ol.getElementsByTagName("li");
		//for (var j = 0, elem_li; elem_li = _li[j]; j++)
		//{
		//	elem_li.innerHTML = elem_li.innerHTML.replace(/(https?:\/\/)([^\s<]+)/,"<a href=\"$1$2\">$2");
		//}
		if(_li.length > 0)
			elem_ol.outerHTML = "<h2>References</h2>"+elem_ol.outerHTML
	}
}
</script>
      <br>
<footer>
  <div class="container">
  <div class="row">
    <div class="col-sm-12" align="center">
      <a href="https://github.com/revbayes">GitHub</a> | <a href="/revbayes_tutorials/license">License</a> | <a href="/revbayes_tutorials/citation">Cite RevBayes</a> | <a href="https://groups.google.com/forum/#!forum/revbayes-users">Users' Forum</a>
    </div>
  </div>
  </div>
</footer>

    </div>
    <script src="/revbayes_tutorials/assets/js/vendor/jquery.min.js"></script>
<script src="/revbayes_tutorials/assets/js/vendor/FileSaver.min.js"></script>
<script src="/revbayes_tutorials/assets/js/vendor/jszip.min.js"></script>
<script src="/revbayes_tutorials/assets/js/vendor/bootstrap.min.js"></script>

<script type="text/javascript">
// Add default language
$(":not(code).highlighter-rouge").each(function() {
  
  if( this.classList == "highlighter-rouge") {
    this.classList = "Rev highlighter-rouge";
  }
  
});
// $("code.highlighter-rouge").each(function() {
//   
//   if( this.classList == "highlighter-rouge") {
//       this.classList = "Rev highlighter-rouge";
//   }
//   
// });
</script>

<script src="/revbayes_tutorials/assets/js/base.js"></script>

<script type="text/javascript"
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      processEscapes: true
    }
  });
</script>

  </body>
</html>
